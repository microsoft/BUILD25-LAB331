{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LAB331: Deep Research with LangChain and DeepSeek R1","text":"<p>Welcome to the Azure Deep Research Workshop! In this hands-on workshop, you'll learn how to build an AI-powered research assistant that can conduct comprehensive web research, analyze and synthesize information, and present findings with illustrative images.</p> <p></p>"},{"location":"#what-youll-build","title":"What You'll Build","text":"<p>This workshop will guide you through creating a sophisticated AI research system that leverages Azure OpenAI's powerful reasoning capabilities, web search integration, and a modern responsive UI to deliver comprehensive research results.</p> <p>By the end of this workshop, you'll have built a complete research application that can:</p> <ul> <li>Process research requests with transparent, real-time AI reasoning</li> <li>Conduct multi-stage web research with knowledge gap identification</li> <li>Perform iterative research cycles to build comprehensive knowledge</li> <li>Present findings with a professional UI including relevant images</li> <li>Deploy as a scalable web application with WebSockets for real-time updates</li> </ul>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>The workshop is organized into four labs:</p> <ol> <li>Reasoning &amp; Model Thoughts: Understand how to harness reasoning models and stream their thinking process in real-time</li> <li>Web Research Integration: Integrate web search capabilities with query generation and information synthesis</li> <li>Research Reflection: Implement knowledge gap identification and iterative research cycles</li> <li>Launching Your Researcher: Transform your terminal application into a modern web interface with FastAPI and WebSockets</li> </ol>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Before starting this workshop, ensure you have:</p> <ul> <li>Python 3.10+ installed on your machine</li> <li>An Azure account with access to Azure OpenAI Service</li> <li>A basic understanding of Python programming</li> <li>Familiarity with web technologies (HTML, CSS, JavaScript)</li> <li>Access to a Tavily API key for web search capability</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To begin the workshop, proceed to the Getting Started section to set up your environment and install the necessary dependencies.</p> <p>Let's start building your AI-powered research assistant!</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section will help you set up your environment for the Azure Deep Research Workshop.</p>"},{"location":"getting-started/#getting-started_1","title":"Getting Started","text":"<p>Select your workshop experience:</p> @Build Workshop <p>{% include-markdown \"includes/introduction-event.md\" %}</p>"},{"location":"lab-1-reasoning-thoughts/","title":"Lab 1: Reasoning &amp; Model Thoughts","text":"<p>In this lab, you'll learn about reasoning models and how to separate and visualize the AI's thinking process from its final output. This approach gives you insight into the model's reasoning and helps create more transparent AI systems.</p>"},{"location":"lab-1-reasoning-thoughts/#understanding-ai-reasoning","title":"Understanding AI Reasoning","text":"<p>Modern large language models like DeepSeek-R1 and GPT-4o have powerful reasoning capabilities, but they often hide their step-by-step thinking. By explicitly encouraging and capturing this thinking process, we can:</p> <ol> <li>Improve the quality of final outputs</li> <li>Debug reasoning errors</li> <li>Build trust through transparency</li> <li>Create educational tools showing \"how\" the AI thinks</li> </ol>"},{"location":"lab-1-reasoning-thoughts/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll:</p> <ol> <li>Learn about reasoning techniques in LLMs</li> <li>Create a simple terminal application that shows thinking vs output</li> <li>Use special prompting techniques to encourage step-by-step reasoning</li> <li>Format the thinking and output visually for better user experience</li> <li>See real-time streaming of the model's thoughts as they develop</li> </ol>"},{"location":"lab-1-reasoning-thoughts/#the-code","title":"The Code","text":"<p>The Python application in <code>lab1_reasoning.py</code> demonstrates reasoning with visible thoughts:</p> <pre><code>import os\nimport time\nfrom dotenv import load_dotenv\nfrom typing import Tuple\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.markdown import Markdown\nfrom rich.prompt import Prompt\nfrom rich.live import Live\n\n# ... abbreviated for clarity ...\n</code></pre> <p>The code creates a terminal-based application that:</p> <ol> <li>Connects to Azure OpenAI using your credentials</li> <li>Takes research questions as input</li> <li>Uses a special system prompt to encourage the model to \"think aloud\"</li> <li>Streams the thinking process in real-time as the model generates it</li> <li>Formats the final answer as bullet points for clarity</li> <li>Presents the results in styled panels</li> </ol>"},{"location":"lab-1-reasoning-thoughts/#key-components","title":"Key Components","text":""},{"location":"lab-1-reasoning-thoughts/#system-prompt","title":"System Prompt","text":"<p>The system prompt instructs the model to show its reasoning:</p> <pre><code>SYSTEM_PROMPT = \"\"\"You are a research assistant that thinks carefully about questions before answering.\n\nWhen you receive a research question, first think about the problem step-by-step.\nPlace all your reasoning and thinking inside &lt;think&gt;...&lt;/think&gt; tags.\n\nAfter thinking, provide your final answer without the thinking tags in bullet points.\nMake sure to include all the important details in your answer.\n\"\"\"\n</code></pre> <p>This prompt: - Establishes the model's role as a research assistant - Provides explicit instructions for structuring output - Uses XML-like tags (<code>&lt;think&gt;...&lt;/think&gt;</code>) to demarcate thinking - Requires bullet points in the final answer for better readability</p>"},{"location":"lab-1-reasoning-thoughts/#real-time-streaming","title":"Real-Time Streaming","text":"<p>The application streams the model's thinking in real-time using the <code>stream_thinking_and_answer</code> function:</p> <pre><code>def stream_thinking_and_answer(stream_generator, title=\"\ud83e\udde0 AI Thinking Process (Live)\"):\n    \"\"\"\n    Stream the AI's thinking and answer in real-time, separating them visually.\n    \"\"\"\n    # Containers for accumulating thoughts and answer\n    accumulated_thoughts = \"\"\n    accumulated_answer = \"\"\n    in_thinking_section = False\n\n    thinking_panel = Panel(\n        Markdown(\"\"),\n        title=title,\n        title_align=\"left\",\n        border_style=\"cyan\",\n        padding=(1, 2),\n        expand=False\n    )\n\n    with Live(thinking_panel, refresh_per_second=4) as live:\n        for chunk in stream_generator:\n            # ... process each token as it arrives ...\n</code></pre> <p>This function: - Creates a live updating panel using Rich's <code>Live</code> context manager  - Processes each token as it comes from the model - Distinguishes between thinking sections and the final answer - Updates the display in real-time with each new token</p>"},{"location":"lab-1-reasoning-thoughts/#running-the-application","title":"Running the Application","text":"<p>To run the application:</p> <ol> <li>Ensure you have the required packages installed:</li> </ol> <pre><code>pip install python-dotenv langchain-azure-ai rich\n</code></pre> <ol> <li>Create a <code>.env</code> file with your Azure OpenAI credentials:</li> </ol> <pre><code>AZURE_AI_ENDPOINT=your_azure_openai_endpoint\nAZURE_API_KEY=your_azure_openai_key\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>python lab1_reasoning.py\n</code></pre> <ol> <li>Enter research questions at the prompt and observe:</li> <li>The model's thinking process streaming live in real-time</li> <li>The final bullet-point answer displayed separately</li> </ol>"},{"location":"lab-1-reasoning-thoughts/#example-usage","title":"Example Usage","text":"<p>When you run the application, you'll see: 1. A prompt asking for your research question 2. A live cyan panel that updates as the model thinks 3. A green panel with the final bullet-point answer</p> <p>Try questions like:</p> <ul> <li>\"What are the environmental impacts of electric vehicles?\"</li> <li>\"How does quantum computing differ from classical computing?\"</li> <li>\"What factors contribute to biodiversity loss in rainforests?\"</li> </ul>"},{"location":"lab-1-reasoning-thoughts/#benefits","title":"Benefits","text":"<p>This approach of streaming reasoning as it happens offers several advantages:</p> <ol> <li>Transparency: Users can see how the model is reasoning step-by-step</li> <li>Engagement: The live updating creates a more dynamic user experience</li> <li>Educational Value: Observe the reasoning patterns of AI in real time</li> <li>Debugging: Identify reasoning flaws as they emerge</li> <li>Trust: The decision-making is no longer a black box</li> </ol>"},{"location":"lab-1-reasoning-thoughts/#lab-challenges","title":"Lab Challenges","text":"<p>Now that you understand the basics, try these challenges:</p> <ol> <li>Modify the System Prompt: Change the prompt to encourage different types of reasoning</li> <li>Enhance the Visualization: Add progress bars or animations to the thinking display</li> <li>Compare Different Models: Test how different models approach the same question</li> <li>Add Error Handling: Improve how the system handles malformed responses or network issues</li> </ol>"},{"location":"lab-1-reasoning-thoughts/#key-takeaways","title":"Key Takeaways","text":"<p>From this lab, you should understand:</p> <ul> <li>How to encourage models to expose their reasoning process</li> <li>Techniques for streaming AI thinking in real-time</li> <li>The value of separating thinking from final outputs</li> <li>How to implement a basic research assistant with visible thinking</li> </ul>"},{"location":"lab-1-reasoning-thoughts/#next-steps","title":"Next Steps","text":"<p>Ready to add web research capabilities to your AI research assistant? Move on to Lab 2: Web Research Integration.</p>"},{"location":"lab-2-web-research/","title":"Lab 2: Web Research Integration","text":"<p>In this lab, you'll learn how to enhance your AI research assistant by integrating web search capabilities. This allows the model to access up-to-date information beyond its training data and provide more accurate and comprehensive research results.</p>"},{"location":"lab-2-web-research/#understanding-web-connected-ai","title":"Understanding Web-Connected AI","text":"<p>While large language models have extensive knowledge, they have two major limitations:</p> <ol> <li>Training Data Cutoff: Models only have knowledge up to their training cutoff date</li> <li>Knowledge Limitations: No model knows everything, especially about niche or emerging topics</li> </ol> <p>By connecting your AI to web search capabilities, you can: - Obtain current information not available during model training - Get authoritative information from reliable sources - Research specialized topics where the model's knowledge may be limited - Find specific data points, statistics, and references</p>"},{"location":"lab-2-web-research/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll:</p> <ol> <li>Set up the Tavily API for web search</li> <li>Create a focused research process with web search integration</li> <li>Structure and analyze search results</li> <li>Build a script that performs research with an AI-guided search strategy</li> <li>See real-time streaming of the AI's thinking at each stage of the process</li> </ol>"},{"location":"lab-2-web-research/#the-web-research-process","title":"The Web Research Process","text":"<p>The web research integration involves three key components:</p> <ol> <li>Query Generation: Creating effective search queries based on the research topic</li> <li>Web Search: Retrieving relevant information from external sources</li> <li>Information Synthesis: Integrating the retrieved information into a cohesive summary</li> </ol> <p></p>"},{"location":"lab-2-web-research/#setting-up-tavily-api","title":"Setting Up Tavily API","text":"<p>In this lab, we'll use the Tavily Search API, which provides AI-optimized web search capabilities. </p> <ol> <li>Sign up for a Tavily API key</li> <li>Add your Tavily API key to your <code>.env</code> file:</li> </ol> <pre><code>TAVILY_API_KEY=your_tavily_api_key\n</code></pre>"},{"location":"lab-2-web-research/#the-code","title":"The Code","text":"<p>The Python application in <code>lab2_web_research.py</code> builds on our reasoning model from Lab 1 and adds web search capabilities:</p> <pre><code>import os\nimport json\nimport time\nimport dotenv\nfrom typing import Dict, List, Any, Tuple\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom tavily import TavilyClient\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.markdown import Markdown\nfrom rich.prompt import Prompt\nfrom rich.progress import Progress, SpinnerColumn, TextColumn\nfrom rich.live import Live\n\n# ... abbreviated for clarity ...\n</code></pre>"},{"location":"lab-2-web-research/#key-components","title":"Key Components","text":""},{"location":"lab-2-web-research/#query-generation","title":"Query Generation","text":"<p>The system first generates an optimal search query based on the research topic:</p> <pre><code>def generate_search_query(research_topic):\n    \"\"\"Generate an effective search query for the research topic.\"\"\"\n    console.print(\"[bold blue]Generating optimal search query...[/]\")\n\n    messages = [\n        SystemMessage(content=QUERY_GENERATION_PROMPT),\n        HumanMessage(content=f\"Research topic: {research_topic}\")\n    ]\n\n    # Stream the model's thinking process\n    console.print(\"\\n[bold]Query Generation Process:[/]\\n\")\n    response_stream = model.stream(messages)\n    thoughts, query = stream_thinking_and_answer(response_stream, \"\ud83d\udd0d Query Generation Thinking\")\n\n    display_panel(f\"**Search Query**: {query}\", \"\ud83d\udd0d Generated Search Query\", \"green\")\n\n    return query\n</code></pre> <p>This function uses a specialized prompt to help the model craft an effective search query, streaming its thinking process in real-time.</p>"},{"location":"lab-2-web-research/#web-search","title":"Web Search","text":"<p>The generated query is used to perform a web search using the Tavily API:</p> <pre><code>def perform_web_search(query):\n    \"\"\"Perform a web search using the Tavily API.\"\"\"\n    console.print(\"\\n[bold blue]Performing web search...[/]\")\n\n    with Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        transient=True,\n    ) as progress:\n        progress.add_task(\"Searching the web...\", total=None)\n        search_results = tavily_client.search(query=query, search_depth=\"advanced\", max_results=5)\n\n    # Display search result snippets\n    console.print(\"\\n[bold]Search Results:[/]\")\n    for i, result in enumerate(search_results[\"results\"], 1):\n        display_panel(\n            f\"**Title**: {result['title']}\\n\\n**Snippet**: {result['content']}\\n\\n**URL**: {result['url']}\",\n            f\"Result {i}\",\n            \"blue\"\n        )\n\n    return search_results\n</code></pre> <p>This retrieves information from the web about the topic and displays snippets of the results.</p>"},{"location":"lab-2-web-research/#information-synthesis","title":"Information Synthesis","text":"<p>The search results are then summarized into a coherent research overview:</p> <pre><code>def summarize_search_results(research_topic, search_results):\n    \"\"\"Summarize the information from search results.\"\"\"\n    console.print(\"\\n[bold blue]Synthesizing information from search results...[/]\")\n\n    # Format search results for the model\n    formatted_results = \"\"\n    for i, result in enumerate(search_results[\"results\"], 1):\n        formatted_results += f\"Source {i}: {result['title']}\\n\"\n        formatted_results += f\"URL: {result['url']}\\n\"\n        formatted_results += f\"Content: {result['content']}\\n\\n\"\n\n    messages = [\n        SystemMessage(content=SUMMARIZATION_PROMPT),\n        HumanMessage(content=f\"Research Topic: {research_topic}\\n\\nSearch Results:\\n{formatted_results}\")\n    ]\n\n    # Stream the model's thinking process for summarization\n    console.print(\"\\n[bold]Summarization Process:[/]\\n\")\n    response_stream = model.stream(messages)\n    thoughts, summary = stream_thinking_and_answer(response_stream, \"\ud83d\udcdd Summarization Thinking\")\n\n    display_panel(summary, \"\ud83d\udcdd Research Summary\", \"green\")\n\n    return summary\n</code></pre> <p>This synthesizes the search results into a comprehensive summary, again streaming the AI's thinking process.</p>"},{"location":"lab-2-web-research/#research-orchestration","title":"Research Orchestration","text":"<p>The <code>conduct_research</code> function ties everything together:</p> <pre><code>def conduct_research(research_topic):\n    \"\"\"Conduct multi-step research on a topic.\"\"\"\n    console.print(f\"[bold]Starting research on: [green]{research_topic}[/green][/]\\n\")\n\n    # Generate search query\n    query = generate_search_query(research_topic)\n\n    # Perform web search\n    search_results = perform_web_search(query)\n\n    # Summarize search results\n    summary = summarize_search_results(research_topic, search_results)\n\n    return summary\n</code></pre> <p>This function orchestrates the complete research process, from query generation to final summary.</p>"},{"location":"lab-2-web-research/#running-the-application","title":"Running the Application","text":"<p>To run the application:</p> <ol> <li>Ensure you have the required packages installed:</li> </ol> <pre><code>pip install python-dotenv langchain-azure-ai tavily-python rich\n</code></pre> <ol> <li>Make sure your <code>.env</code> file includes both Azure OpenAI and Tavily API keys:</li> </ol> <pre><code>AZURE_AI_ENDPOINT=your_azure_openai_endpoint\nAZURE_API_KEY=your_azure_openai_key\nTAVILY_API_KEY=your_tavily_api_key\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>python lab2_web_research.py\n</code></pre> <ol> <li>Enter a research topic and observe the complete research process.</li> </ol>"},{"location":"lab-2-web-research/#example-usage","title":"Example Usage","text":"<p>When you run the application and enter a research topic like \"latest advancements in fusion energy\", you'll see:</p> <ol> <li>The model generating an effective search query with visible thinking</li> <li>Results from the web search with titles and snippets</li> <li>The model synthesizing a summary with visible thinking</li> <li>A final comprehensive research summary</li> </ol>"},{"location":"lab-2-web-research/#lab-challenges","title":"Lab Challenges","text":"<p>Now that you understand web research integration, try these challenges:</p> <ol> <li>Modify the Search Depth: Change the Tavily search parameters to get different results</li> <li>Customize the Prompts: Adjust the prompts to generate different types of summaries</li> <li>Add Image Support: Extend the script to collect and display relevant images from the web</li> <li>Source Citation: Enhance the summary to include proper citations for each piece of information</li> </ol>"},{"location":"lab-2-web-research/#key-takeaways","title":"Key Takeaways","text":"<p>From this lab, you should understand:</p> <ul> <li>How to connect AI models to external knowledge sources</li> <li>Techniques for generating effective search queries</li> <li>Methods for synthesizing information from multiple sources</li> <li>The power of real-time thinking visualization during each stage</li> </ul>"},{"location":"lab-2-web-research/#next-steps","title":"Next Steps","text":"<p>Ready to extend your research assistant with knowledge gap identification and iterative research? Move on to Lab 3: Research Reflection.</p>"},{"location":"lab-3-reflection/","title":"Lab 3: Research Reflection","text":"<p>In this lab, you'll learn how to create a more thorough research system by implementing knowledge gap identification and iterative research cycles. This advanced approach allows your research assistant to identify what it doesn't know and conduct follow-up research to create more comprehensive results.</p>"},{"location":"lab-3-reflection/#understanding-research-reflection","title":"Understanding Research Reflection","text":"<p>Even with web search capabilities, a single research cycle often leaves important questions unanswered. Real research is iterative - findings from initial searches reveal what else we need to know.</p> <p>Research reflection involves:</p> <ol> <li>Knowledge Gap Identification: Analyzing current information to find what's missing</li> <li>Follow-up Query Generation: Creating targeted follow-up questions to fill these gaps</li> <li>Multiple Research Cycles: Conducting iterative research to build comprehensive knowledge</li> <li>Progressive Synthesis: Combining findings from all iterations into a coherent whole</li> </ol>"},{"location":"lab-3-reflection/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll:</p> <ol> <li>Build upon the web search capabilities from Lab 2</li> <li>Implement knowledge gap analysis with AI reasoning</li> <li>Create an iterative research system that conducts multiple search cycles</li> <li>Generate comprehensive research reports combining multiple iterations</li> <li>See real-time streaming of the AI's thinking throughout the entire process</li> </ol>"},{"location":"lab-3-reflection/#the-iterative-research-process","title":"The Iterative Research Process","text":"<p>The complete iterative research process includes:</p> <ol> <li>Initial query generation and web search (from Lab 2)</li> <li>Knowledge gap identification after initial research</li> <li>Follow-up search cycles based on identified gaps</li> <li>Synthesis of all findings into a comprehensive report</li> </ol>"},{"location":"lab-3-reflection/#the-code","title":"The Code","text":"<p>The Python application in <code>lab3_reflection.py</code> extends our Lab 2 implementation with knowledge gap identification and iterative research:</p> <pre><code>import os\nimport json\nimport time\nimport dotenv\nfrom typing import Dict, List, Any, Tuple\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom tavily import TavilyClient\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.markdown import Markdown\nfrom rich.prompt import Prompt\nfrom rich.progress import Progress, SpinnerColumn, TextColumn\nfrom rich.live import Live\n\n# ... abbreviated for clarity ...\n</code></pre>"},{"location":"lab-3-reflection/#key-components","title":"Key Components","text":""},{"location":"lab-3-reflection/#knowledge-gap-identification","title":"Knowledge Gap Identification","text":"<p>After the initial research cycle, the system analyzes what information is missing:</p> <pre><code>def identify_knowledge_gaps(research_topic, current_summary):\n    \"\"\"Identify knowledge gaps and generate a follow-up query.\"\"\"\n    console.print(\"\\n[bold blue]Identifying knowledge gaps...[/]\")\n\n    messages = [\n        SystemMessage(content=REFLECTION_PROMPT),\n        HumanMessage(content=f\"Research Topic: {research_topic}\\n\\nCurrent Summary:\\n{current_summary}\")\n    ]\n\n    # Stream the model's thinking process for knowledge gap identification\n    console.print(\"\\n[bold]Knowledge Gap Analysis Process:[/]\\n\")\n    response_stream = model.stream(messages)\n    thoughts, json_str = stream_thinking_and_answer(response_stream, \"\ud83d\udd0d Reflection Thinking\")\n\n    # Try to parse the JSON response\n    try:\n        reflection = json.loads(json_str)\n        knowledge_gap = reflection.get(\"knowledge_gap\", \"No specific knowledge gap identified.\")\n        follow_up_query = reflection.get(\"follow_up_query\", f\"More information about {research_topic}\")\n    except json.JSONDecodeError:\n        # Fallback if JSON parsing fails\n        knowledge_gap = \"Unable to parse the identified knowledge gap.\"\n        follow_up_query = f\"More information about {research_topic}\"\n\n    display_panel(\n        f\"**Knowledge Gap**: {knowledge_gap}\\n\\n**Follow-up Query**: {follow_up_query}\",\n        \"\ud83d\udd0d Knowledge Gap Analysis\",\n        \"yellow\"\n    )\n\n    return knowledge_gap, follow_up_query\n</code></pre> <p>This function: - Analyzes the current research summary to identify knowledge gaps - Generates a specific follow-up query to address the most important gap - Returns both the knowledge gap description and the follow-up query - Streams the model's thinking process in real-time</p>"},{"location":"lab-3-reflection/#multi-iteration-research","title":"Multi-Iteration Research","text":"<p>The <code>conduct_research</code> function now performs multiple research iterations:</p> <pre><code>def conduct_research(research_topic, max_iterations=2):\n    \"\"\"Conduct multi-step research on a topic.\"\"\"\n    console.print(f\"[bold]Starting research on: [green]{research_topic}[/green][/]\\n\")\n\n    all_summaries = []\n\n    for iteration in range(1, max_iterations + 1):\n        console.print(f\"\\n[bold]===== Research Iteration {iteration} =====[/]\\n\")\n\n        # Generate search query (use the research topic for first iteration)\n        if iteration == 1:\n            query = generate_search_query(research_topic)\n        else:\n            # For subsequent iterations, use the follow-up query from reflection\n            _, query = identify_knowledge_gaps(research_topic, all_summaries[-1])\n\n        # Perform web search\n        search_results = perform_web_search(query)\n\n        # Summarize search results\n        summary = summarize_search_results(research_topic, search_results)\n        all_summaries.append(summary)\n\n    # Compile final research report from all summaries\n    console.print(\"\\n[bold]===== Final Research Report =====[/]\\n\")\n\n    final_report = f\"# Research Report: {research_topic}\\n\\n\"\n    for i, summary in enumerate(all_summaries, 1):\n        final_report += f\"## Research Cycle {i}\\n\\n{summary}\\n\\n\"\n\n    display_panel(final_report, \"\ud83d\udcca Complete Research Report\", \"purple\")\n\n    return final_report\n</code></pre> <p>This function: - Performs multiple research iterations (default is 2) - Uses the initial topic for the first search - Uses knowledge gap analysis to guide subsequent searches - Compiles all findings into a comprehensive final report</p>"},{"location":"lab-3-reflection/#research-report-compilation","title":"Research Report Compilation","text":"<p>The final report combines findings from all research iterations:</p> <pre><code># Compile final research report from all summaries\nconsole.print(\"\\n[bold]===== Final Research Report =====[/]\\n\")\n\nfinal_report = f\"# Research Report: {research_topic}\\n\\n\"\nfor i, summary in enumerate(all_summaries, 1):\n    final_report += f\"## Research Cycle {i}\\n\\n{summary}\\n\\n\"\n\ndisplay_panel(final_report, \"\ud83d\udcca Complete Research Report\", \"purple\")\n</code></pre> <p>This creates a well-structured report with sections for each research cycle.</p>"},{"location":"lab-3-reflection/#running-the-application","title":"Running the Application","text":"<p>To run the application:</p> <ol> <li>Ensure you have the required packages installed:</li> </ol> <pre><code>pip install python-dotenv langchain-azure-ai tavily-python rich\n</code></pre> <ol> <li>Make sure your <code>.env</code> file includes both Azure OpenAI and Tavily API keys:</li> </ol> <pre><code>AZURE_AI_ENDPOINT=your_azure_openai_endpoint\nAZURE_API_KEY=your_azure_openai_key\nTAVILY_API_KEY=your_tavily_api_key\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>python lab3_reflection.py\n</code></pre> <ol> <li>Enter a research topic and observe the multi-cycle research process.</li> </ol>"},{"location":"lab-3-reflection/#example-usage","title":"Example Usage","text":"<p>When you run the application and enter a research topic like \"quantum computing applications in medicine\", you'll see:</p> <ol> <li>First Research Cycle:</li> <li>Generation of an initial search query</li> <li>Web search results</li> <li> <p>Summary of initial findings</p> </li> <li> <p>Knowledge Gap Analysis:</p> </li> <li>Identification of missing information</li> <li> <p>Generation of a follow-up query</p> </li> <li> <p>Second Research Cycle:</p> </li> <li>Web search using the follow-up query</li> <li> <p>Summary of additional findings</p> </li> <li> <p>Final Research Report:</p> </li> <li>Comprehensive report combining all research cycles</li> </ol>"},{"location":"lab-3-reflection/#benefits-of-iterative-research","title":"Benefits of Iterative Research","text":"<p>This approach offers several advantages:</p> <ol> <li>Thoroughness: Multiple research cycles produce more comprehensive results</li> <li>Targeted Follow-up: Each cycle focuses on what's still unknown</li> <li>Progressive Refinement: Later cycles build on knowledge from earlier ones</li> <li>Better Coverage: Different search queries capture different aspects of the topic</li> <li>Self-Awareness: The system acknowledges what it doesn't know</li> </ol>"},{"location":"lab-3-reflection/#lab-challenges","title":"Lab Challenges","text":"<p>Try these challenges to extend your learning:</p> <ol> <li>Increase Iterations: Modify the code to perform 3 or more research cycles</li> <li>Topic Segmentation: Modify the system to explore different subtopics in parallel</li> <li>User Guidance: Add user input between iterations to guide the research direction</li> <li>Sentiment Analysis: Add analysis of different perspectives on controversial topics</li> <li>Citation Improvement: Create a more formal citation system for sources</li> </ol>"},{"location":"lab-3-reflection/#key-takeaways","title":"Key Takeaways","text":"<p>From this lab, you should understand:</p> <ul> <li>How to implement knowledge gap identification</li> <li>Techniques for creating targeted follow-up queries</li> <li>Methods for conducting multi-cycle research</li> <li>The value of iterative approaches in comprehensive research</li> </ul>"},{"location":"lab-3-reflection/#next-steps","title":"Next Steps","text":"<p>Ready to incorporate this research system into a full production application? Move on to Lab 4: Launching Your Researcher.</p>"},{"location":"lab-4-launch-researcher/","title":"Lab 4: Deploying Your Research Assistant","text":"<p>In this lab, you'll transform your terminal-based research assistant into a professional web application using FastAPI and WebSockets. This web interface provides a more user-friendly experience with real-time research updates and a polished UI.</p>"},{"location":"lab-4-launch-researcher/#understanding-web-application-deployment","title":"Understanding Web Application Deployment","text":"<p>Moving from a command-line tool to a web application offers several advantages:</p> <ol> <li>User Accessibility: Anyone can use the application through a web browser</li> <li>Real-time Updates: WebSockets enable live streaming of research progress</li> <li>Professional Presentation: A well-designed UI enhances the research experience</li> <li>Scalability: The application can be deployed to cloud services for broader access</li> </ol>"},{"location":"lab-4-launch-researcher/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll:</p> <ol> <li>Learn about the FastAPI web framework</li> <li>Set up a WebSocket connection for real-time streaming</li> <li>Create a research workflow using a state graph</li> <li>Deploy a responsive web interface</li> <li>See all previous lab techniques integrated into a production-ready application</li> </ol>"},{"location":"lab-4-launch-researcher/#architecture-overview","title":"Architecture Overview","text":"<p>The web application uses a modern architecture:</p> <ol> <li>Backend: FastAPI Python server with WebSocket support</li> <li>Frontend: HTML/CSS/JavaScript with Tailwind CSS for styling</li> <li>State Management: LangGraph for structured AI workflow orchestration</li> <li>Data Flow: Real-time bidirectional communication via WebSockets</li> </ol> <p></p>"},{"location":"lab-4-launch-researcher/#key-components","title":"Key Components","text":""},{"location":"lab-4-launch-researcher/#fastapi-application","title":"FastAPI Application","text":"<p>The main application in <code>app/main.py</code> serves as the backend server:</p> <pre><code>from fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom langgraph.graph import StateGraph, START, END\n# ... other imports ...\n\napp = FastAPI(title=\"Azure Deep Research\")\n\n# Mount static files\napp.mount(\"/static\", StaticFiles(directory=\"app/static\"), name=\"static\")\n\n# Set up templates\ntemplates = Jinja2Templates(directory=\"app/templates\")\n</code></pre>"},{"location":"lab-4-launch-researcher/#research-graph-with-langgraph","title":"Research Graph with LangGraph","text":"<p>The application uses LangGraph to create a structured research workflow:</p> <pre><code>def setup_graph():\n    # Add nodes and edges\n    builder = StateGraph(SummaryState, input=SummaryStateInput, output=SummaryStateOutput)\n    builder.add_node(\"generate_query\", generate_query)\n    builder.add_node(\"web_research\", web_research)\n    builder.add_node(\"summarize_sources\", summarize_sources)\n    builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n    builder.add_node(\"finalize_summary\", finalize_summary)\n\n    # Add edges\n    builder.add_edge(START, \"generate_query\")\n    builder.add_edge(\"generate_query\", \"web_research\")\n    builder.add_edge(\"web_research\", \"summarize_sources\")\n    builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n    builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n    builder.add_edge(\"finalize_summary\", END)\n\n    return builder.compile()\n</code></pre> <p>This creates a research graph with: 1. Query generation 2. Web research retrieval 3. Information summarization 4. Knowledge gap reflection 5. Multiple research cycles 6. Final report generation</p>"},{"location":"lab-4-launch-researcher/#websocket-communication","title":"WebSocket Communication","text":"<p>Real-time updates are provided through WebSockets:</p> <pre><code>@app.websocket(\"/ws/{client_id}\")\nasync def websocket_endpoint(websocket: WebSocket, client_id: str):\n    await websocket.accept()\n    manager.connect(websocket, client_id)\n    try:\n        # Set up the graph\n        graph = setup_graph()\n\n        while True:\n            data = await websocket.receive_text()\n            data_json = json.loads(data)\n\n            if data_json.get(\"type\") == \"research\":\n                # ... start research process ...\n\n                async def stream_graph_updates():\n                    # ... stream updates to client ...\n\n                # Run graph execution in the background\n                asyncio.create_task(stream_graph_updates())\n\n    except WebSocketDisconnect:\n        manager.disconnect(client_id)\n</code></pre>"},{"location":"lab-4-launch-researcher/#interactive-web-interface","title":"Interactive Web Interface","text":"<p>The web interface in <code>app/templates/index.html</code> provides a clean, responsive UI:</p> <ul> <li>Research topic input</li> <li>Real-time progress indicators</li> <li>Live streaming of research steps</li> <li>Formatted research report display</li> <li>\"AI Thinking\" modal to show reasoning process</li> </ul>"},{"location":"lab-4-launch-researcher/#running-the-application","title":"Running the Application","text":"<p>To launch the web application:</p> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the FastAPI server:</li> </ol> <pre><code>uvicorn app.main:app --reload\n</code></pre> <ol> <li>Open your browser and navigate to:</li> </ol> <pre><code>http://localhost:8000\n</code></pre>"},{"location":"lab-4-launch-researcher/#using-the-web-interface","title":"Using the Web Interface","text":"<p>The web application features a clean, intuitive interface:</p> <ol> <li>Research Input: Enter a research topic in the input field and click \"Research\"</li> <li>Progress Tracking: Watch as the system progresses through each research step</li> <li>Live Updates: See real-time updates as the research is conducted</li> <li>Thinking Process: Click the thought bubble icon to view the AI's reasoning process</li> <li>Final Report: View the comprehensive research report with citations</li> </ol>"},{"location":"lab-4-launch-researcher/#behind-the-scenes","title":"Behind the Scenes","text":"<p>When a user submits a research topic, the application:</p> <ol> <li>Initializes a WebSocket connection for real-time communication</li> <li>Creates a research graph using LangGraph</li> <li>Executes the research workflow in a structured way:</li> <li>Generates an effective search query with visible thinking</li> <li>Performs web searches using the Tavily API</li> <li>Synthesizes information from search results</li> <li>Identifies knowledge gaps and creates follow-up queries</li> <li>Conducts multiple research cycles (up to 3 by default)</li> <li>Compiles a final research report with images and sources</li> <li>Streams progress updates to the client in real-time</li> <li>Displays the final research report with proper formatting</li> </ol>"},{"location":"lab-4-launch-researcher/#deployment-options","title":"Deployment Options","text":"<p>After testing locally, you can deploy the application to various platforms:</p>"},{"location":"lab-4-launch-researcher/#azure-app-service","title":"Azure App Service","text":"<ol> <li>Create an Azure App Service with Python support</li> <li>Set up your environment variables in the App Service configuration</li> <li>Deploy your code using Azure DevOps, GitHub Actions, or direct deployment</li> </ol>"},{"location":"lab-4-launch-researcher/#docker-deployment","title":"Docker Deployment","text":"<ol> <li>Create a Dockerfile in your project root:</li> </ol> <pre><code>FROM python:3.10\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <ol> <li>Build and run the Docker container:</li> </ol> <pre><code>docker build -t azure-deep-research .\ndocker run -p 8000:8000 -d azure-deep-research\n</code></pre>"},{"location":"lab-4-launch-researcher/#enhancing-your-application","title":"Enhancing Your Application","text":"<p>Consider these enhancements for your research assistant:</p> <ol> <li>User Authentication: Add user accounts and save research history</li> <li>Database Integration: Store research reports for later reference</li> <li>Research Export: Add options to export as PDF or other formats</li> <li>Custom Research Settings: Allow users to adjust research depth and sources</li> <li>Document Upload: Enable research based on uploaded documents</li> </ol>"},{"location":"lab-4-launch-researcher/#lab-challenges","title":"Lab Challenges","text":"<p>Try these challenges to extend your learning:</p> <ol> <li>Add Visualization: Integrate data visualization for numerical research topics</li> <li>Implement Caching: Add result caching to improve performance</li> <li>Multi-Language Support: Add translation capabilities for multiple languages</li> <li>Citation Management: Implement formal academic citation formats</li> <li>Image Analysis: Add capabilities to analyze images found during research</li> </ol>"},{"location":"lab-4-launch-researcher/#key-takeaways","title":"Key Takeaways","text":"<p>From this lab, you should understand:</p> <ul> <li>How to deploy an AI research assistant as a web application</li> <li>Using WebSockets for real-time communication</li> <li>Implementing a structured workflow with LangGraph</li> <li>Creating an interactive user interface for research</li> <li>Integrating all previous lab techniques into a production application</li> </ul>"},{"location":"lab-4-launch-researcher/#congratulations","title":"Congratulations!","text":"<p>You've successfully built and deployed a sophisticated AI research assistant using Azure OpenAI, LangGraph, and FastAPI. This application demonstrates modern AI techniques including:</p> <ul> <li>Transparent thinking and reasoning</li> <li>Web search integration</li> <li>Knowledge gap identification</li> <li>Iterative research cycles</li> <li>Real-time progress streaming</li> </ul> <p>Your application now provides a powerful research tool that can help users explore any topic with the assistance of advanced AI capabilities.</p>"},{"location":"includes/introduction-event/","title":"Introduction event","text":""},{"location":"includes/introduction-event/#microsoft-build-attendees","title":"Microsoft Build Attendees","text":"<p>The instructions on this page assume you are attending Microsoft Build 2025 and have access to a pre-configured lab environment. This environment provides an Azure subscription with all the tools and resources needed to complete the workshop.</p>"},{"location":"includes/introduction-event/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you how to use Reasoning Models, like DeepSeek R1 and utilize tools and Reflection style architecture with LangChain to do deep research. It consists of multiple labs, each highlighting a specific feature of the process of building a deep researcher. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"includes/introduction-event/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure so the agent app can access the Azure AI Agents Service and models. Follow these steps:</p> <ol> <li> <p>Open a terminal window. The terminal app is pinned to the Windows 11 taskbar.</p> <p></p> </li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account.</p> <ol> <li> <p>A browser window will open automatically, select Work or school account and click Next.</p> </li> <li> <p>Use the Username and Password found in the top section of the Resources tab in the lab environment.</p> </li> <li> <p>Select OK, then Done.</p> </li> </ol> </li> <li> <p>Then select the Default subscription from the command line.</p> </li> <li> <p>Once you've logged in, run the following command to assign the user role to the resource group:</p> <pre><code>$subId = $(az account show --query id --output tsv) `\n;$objectId = $(az ad signed-in-user show --query id -o tsv) `\n; az role assignment create --role \"f6c7c914-8db3-469d-8ca1-694a8f32e121\" --assignee-object-id $objectId --scope /subscriptions/$subId/resourceGroups/\"rg-agent-workshop\" --assignee-principal-type 'User'\n</code></pre> </li> <li> <p>Leave the terminal window open for the next steps.</p> </li> </ol>"},{"location":"includes/introduction-event/#open-the-workshop","title":"Open the Workshop","text":"<p>Follow these steps to open the workshop in Visual Studio Code:</p> Python <ol> <li> <p>From the terminal window, execute the following commands to clone the workshop repository, navigate to the relevant folder, set up a virtual environment, activate it, and install the required packages:</p> <pre><code>git clone https://github.com/microsoft/BUILD25-LAB331.git `\n; cd src `\n; python -m venv src/python/workshop/.venv `\n; src\\.venv\\Scripts\\activate `\n; pip install -r src/requirements.txt `\n</code></pre> </li> <li> <p>Open in VS Code. From the terminal window, run the following command:</p> <pre><code>code .vscode\\python-workspace.code-workspace\n</code></pre> <p>When the project opens in VS Code, two notifications appear in the bottom right corner. Click \u2716 to close both notifications.</p> </li> </ol>"},{"location":"includes/introduction-event/#project-connection-string","title":"Project Connection String","text":"<p>Next, we log in to Azure AI Foundry to retrieve the project connection string, which the agent app uses to connect to the Azure AI Agents Service.</p> <ol> <li>Navigate to the Azure AI Foundry website.</li> <li>Select Sign in and use the Username and Password found in the top section of the Resources tab in the lab environment. Click on the Username and Password fields to automatically fill in the login details.     </li> <li>Read the introduction to the Azure AI Foundry and click Got it.</li> <li> <p>Ensure you are on the AI Foundry home page. Click the AI Foundry tab in the top left corner.</p> <p></p> </li> <li> <p>Select the project name that starts with aip-.</p> <p></p> </li> <li> <p>Review the introduction guide and click Close.</p> </li> <li> <p>Locate the Project details section, click the Copy icon to copy the Project connection string.</p> <p></p> </li> </ol>"},{"location":"includes/introduction-event/#configure-the-workshop","title":"Configure the Workshop","text":"<pre><code>1. Switch back to the workshop you opened in VS Code.\n2. **Rename** the `.env.sample` file to `.env`.\n\n    - Select the **.env.sample** file in the VS Code **Explorer** panel.\n    - Right-click the file and select **Rename**, or press &lt;kbd&gt;F2&lt;/kbd&gt;.\n    - Change the file name to `.env` and press &lt;kbd&gt;Enter&lt;/kbd&gt;.\n\n3. Paste the **Project connection string** you copied from Azure AI Foundry into the `.env` file.\n\n    ```python\n    PROJECT_CONNECTION_STRING=\"&lt;your_project_connection_string&gt;\"\n    ```\n\n    Your `.env` file should look similar to this but with your project connection string.\n\n    ```python\n    MODEL_DEPLOYMENT_NAME=\"DeepSeek-R1\"\n    PROJECT_CONNECTION_STRING=\"&lt;your_project_connection_string&gt;\"\n    ```\n\n4. Save the `.env` file.\n</code></pre>"},{"location":"includes/introduction-event/#5-set-up-tavily-api","title":"5. Set Up Tavily API","text":"<pre><code>1. Register for a [Tavily API key](https://tavily.com/) to enable web search capabilities. This API is essential for the web research integration in Labs 2 and 3.\n\n2. Update the `.env` file in the project root with your Tavily API keys. Your file should now look like this:\n\n```\nAZURE_AI_ENDPOINT=your_azure_openai_endpoint\nAZURE_API_KEY=your_azure_openai_key\nTAVILY_API_KEY=your_tavily_api_key\n```\n</code></pre>"},{"location":"includes/introduction-event/#workshop-materials","title":"Workshop Materials","text":"<p>This workshop is designed as a progressive series of labs, each building on the previous one:</p> <ol> <li> <p>Lab 1 (Reasoning &amp; Model Thoughts): Learn how to stream the AI's thinking process in real-time and format final answers as bullet points.</p> </li> <li> <p>Lab 2 (Web Research Integration): Build a basic research system that generates queries, searches the web, and synthesizes the results.</p> </li> <li> <p>Lab 3 (Research Reflection): Extend the research capabilities with knowledge gap identification and iterative research cycles.</p> </li> <li> <p>Lab 4 (Launching Your Researcher): Deploy the research assistant as a web application with FastAPI and WebSockets for real-time updates.</p> </li> </ol> <p>Each lab includes code samples, explanations, and challenges to extend your learning.</p>"},{"location":"includes/introduction-event/#navigation","title":"Navigation","text":"<p>Use the navigation menu on the left to move between labs. Each lab includes:</p> <ul> <li>An overview of what you'll learn</li> <li>Prerequisites specific to that lab</li> <li>Detailed instructions with code samples</li> <li>Explanations of key components</li> <li>Challenges to extend your learning</li> <li>Next steps to progress through the workshop</li> </ul>"},{"location":"includes/introduction-event/#pro-tips","title":"Pro Tips","text":"<p>Tips</p> <ol> <li>The Burger Menu in the right-hand panel of the lab environment offers additional features, including the Split Window View and the option to end the lab. The Split Window View allows you to maximize the lab environment to full screen, optimizing screen space. The lab's Instructions and Resources panel will open in a separate window.</li> <li>If the lab instructions are slow to scroll in the lab environment, try copying the instructions\u2019 URL and opening it in your computer\u2019s local browser for a smoother experience.</li> <li>If you have trouble viewing an image, simply click the image to enlarge it.</li> </ol>"},{"location":"includes/introduction-event/#next-steps","title":"Next Steps","text":"<p>Once your environment is set up, proceed to Lab 1: Reasoning &amp; Model Thoughts to begin building your research assistant.</p>"},{"location":"includes/introduction-self-guided/","title":"Introduction self guided","text":""},{"location":"includes/introduction-self-guided/#self-guided-learners","title":"Self-Guided Learners","text":"<p>These instructions are for self-guided learners who do not have access to a pre-configured lab environment. Follow these steps to set up your environment and begin the workshop.</p>"},{"location":"includes/introduction-self-guided/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you how to use Reasoning Models, like DeepSeek R1 and utilize tools and Reflection style architecture with LangChain to do deep research. It consists of multiple labs, each highlighting a specific feature of the process of building a deep researcher. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"includes/introduction-self-guided/#prerequisites","title":"Prerequisites","text":"<ol> <li>Access to an Azure subscription. If you don't have an Azure subscription, create a free account before you begin.</li> <li>You need a GitHub account. If you don\u2019t have one, create it at GitHub.</li> </ol>"},{"location":"includes/introduction-self-guided/#open-the-workshop","title":"Open the Workshop","text":"<p>The preferred way to run this workshop is using GitHub Codespaces. This option provides a pre-configured environment with all the tools and resources needed to complete the workshop. Alternatively, you can open the workshop locally using a Visual Studio Code Dev Container.</p> GitHub Codespaces <p>Select Open in GitHub Codespaces to open the project in GitHub Codespaces.</p> <p></p> <p>Building the Codespace will take several minutes. You can continue reading the instructions while it builds.</p>"},{"location":"includes/introduction-self-guided/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure so the agent app can access the Azure AI Agents Service and models. Follow these steps:</p> <ol> <li>Ensure the Codespace has been created.</li> <li>In the Codespace, open a new terminal window by selecting Terminal &gt; New Terminal from the VS Code menu.</li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login --use-device-code\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account. Be sure to copy the authentication code first.</p> <ol> <li>A browser window will open automatically, select your account type and click Next.</li> <li>Sign in with your Azure subscription Username and Password.</li> <li>Paste the authentication code.</li> <li>Select OK, then Done.</li> </ol> <p>Warning</p> <p>If you have multiple Azure tenants, then you will need to select the appropriate tenant when authenticating.</p> <pre><code>az login --use-device-code --tenant &lt;tenant_id&gt;\n</code></pre> </li> <li> <p>Next, select the appropriate subscription from the command line.</p> </li> <li>Leave the terminal window open for the next steps.</li> </ol>"},{"location":"includes/introduction-self-guided/#deploy-the-azure-resources","title":"Deploy the Azure Resources","text":"<p>The following resources will be created in the <code>rg-deep-research-workshop</code> resource group in your Azure subscription.</p> <ul> <li>An Azure AI Foundry hub named deep-research-wksp</li> <li>An Azure AI Foundry project named Deep Research Workshop</li> <li>A Serverless (pay-as-you-go) DeepSeek R1 model deployment named gpt-4o (Global 2024-08-06). See pricing details here.</li> </ul> <p>We have provided a bash script to automate the deployment of the resources required for the workshop. Alternatively, you may deploy resources manually using Azure AI Foundry studio. Select the desired tab.</p>"}]}