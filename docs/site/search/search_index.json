{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LAB331: Deep Research with LangChain and DeepSeek R1","text":"<p>Welcome to Lab 331! In this hands-on workshop, you'll learn how 'reasoning models' like DeepSeek R1 work. We'll walk through how to build an AI-powered research assistant that can conduct comprehensive web research, analyze and synthesize information, and present findings with illustrative images. The researcher's API will be built using Langchain Azure AI, DeepSeek R1, LangGraph (Langchain's agentic framework) and FastAPI.  </p> <p></p>"},{"location":"#what-youll-build","title":"What You'll Build","text":"<p>By the end of this workshop, you'll have learnt:</p> <ul> <li>What a Reasoning Model is and how to use one</li> <li>How to use Reasoning Models with tools like Tavily web search for optimum search results</li> <li>What LangGraph is and how to implement reflection style archictecture with it</li> <li>How to use LangGraph to perform iterative research cycles to build comprehensive knowledge</li> <li>How to deploy the final application</li> </ul>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>The workshop is organized into four labs:</p> <ol> <li>Introduction to Reasoning Models</li> <li>Web Research Integration</li> <li>Research Reflection</li> <li>Launching Your Researcher</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To begin the workshop, proceed to the Getting Started section to set up your environment and install the necessary dependencies.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#microsoft-build-attendees","title":"Microsoft Build Attendees","text":"<p>The instructions on this page assume you are attending Microsoft Build 2025 and have access to a pre-configured lab environment. This environment provides an Azure subscription with all the tools and resources needed to complete the workshop.</p>"},{"location":"getting-started/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you how to use Reasoning Models, like DeepSeek R1 and utilize tools and Reflection style architecture with LangChain to do deep research. It consists of multiple labs, each highlighting a specific feature of the process of building a deep researcher. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"getting-started/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure to access DeepSeek R1. Follow these steps:</p> <ol> <li> <p>Open a terminal window. The terminal app is pinned to the Windows taskbar.</p> <p></p> </li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account.</p> <ol> <li> <p>A browser window will open automatically, select Work or school account and click Next.</p> </li> <li> <p>Use the Username and Password found in the top section of the Resources tab in the lab environment.</p> </li> <li> <p>Select OK, then Done.</p> </li> </ol> </li> <li> <p>Then select the Default subscription from the command line.</p> </li> <li> <p>Stay in the terminal window for the next step.</p> </li> </ol>"},{"location":"getting-started/#open-the-workshop-and-setup-your-tavily-account","title":"Open the Workshop and setup your Tavily account","text":"<p>Follow these steps to open the workshop in Visual Studio Code and set up your Tavily account for web search:</p> <ol> <li> <p>From the terminal window, execute the following commands to clone the workshop repository, navigate to the relevant folder, set up a virtual environment, activate it, and install the required packages:</p> <pre><code>git clone https://github.com/microsoft/BUILD25-LAB331.git `\n; cd BUILD25-LAB331 `\n; python -m venv src/.venv `\n; src\\.venv\\Scripts\\activate `\n; pip install -r src/requirements.txt `\n; cd src `\n;\n</code></pre> <p>This command will take a few minutes to complete. While you wait, navigate to the Tavily home page that should already be opened as a tab in your browser. </p> </li> <li> <p>We will be using Tavily to give our deep researcher access to the internet. Once on the Tavily page, click the sign up button and create an account. </p> <p>Note</p> <p>You will need to use either your personal email address or your Github account to sign up since email requires verification. </p> <p>Click the signup button and complete the sign up process </p> <p></p> <p>Once sign up is complete you should see a page with an API Key that looks like this. You will use this key later, so minimize the page for now and return to the terminal. </p> <p></p> </li> <li> <p>From the terminal window, run the following command to open the project in VS Code (note there is a period after the word code. Do not just type in 'code', the period is important.):</p> <pre><code>code .\n</code></pre> <p>When the project opens in VS Code, two notifications appear in the bottom right corner. Click \u2716 to close both notifications.</p> </li> </ol>"},{"location":"getting-started/#configure-the-workshop","title":"Configure the Workshop","text":""},{"location":"getting-started/#create-the-env-file","title":"Create the .env file","text":"<ol> <li> <p>Open a new terminal in VSCode. </p> </li> <li> <p>To create a <code>.env</code> file with the variables needed for this workshop click on the <code>instructions</code> tab in your Skillable lab manual. Click on the command under Lab Guide and patse it in the terminal. Press enter to run the command and follow the instructions.</p> </li> <li>Check that your .env file has succesfully been created and contains some variables. If not, raise your hand and ask a proctor for help.</li> <li>Navigate back to the Tavily API page you should have open in your browser. Copy the API Key and paste it in the .env file as the <code>TAVILY_API_KEY</code> value.</li> </ol> <p>You can now begin with Lab 1! </p>"},{"location":"getting-started/#navigation","title":"Navigation","text":"<p>Use the navigation menu on the left to move between labs.</p>"},{"location":"getting-started/#pro-tips","title":"Pro Tips","text":"<p>Tips</p> <ol> <li>The Burger Menu in the right-hand panel of the lab environment offers additional features, including the Split Window View and the option to end the lab. The Split Window View allows you to maximize the lab environment to full screen, optimizing screen space. The lab's Instructions and Resources panel will open in a separate window.</li> <li>If the lab instructions are slow to scroll in the lab environment, try copying the instructions\u2019 URL and opening it in your computer\u2019s local browser for a smoother experience.</li> <li>If you have trouble viewing an image, simply click the image to enlarge it.</li> </ol>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Once your environment is set up, proceed to Lab 1: Reasoning &amp; Model Thoughts to begin building your research assistant.</p>"},{"location":"lab-1-introduction-to-reasoning-models/","title":"Lab 1: Introduction to Reasoning Models","text":""},{"location":"lab-1-introduction-to-reasoning-models/#what-is-a-reasoning-model","title":"What is a Reasoning Model?","text":"<p>Reasoning models, like DeepSeek-R1, are LLMs trained with reinforcement learning to perform reasoning. These models think before they answer, producing a long internal chain of thought before responding to the user (source.) </p> <p>When combined with tools to access the web, reasoning models are particularly good at research. Their chain\u2011of\u2011thought training and reward signals make them better at spotting gaps, cross\u2011checking sources, and iterating on hypotheses. </p> <p>Some popular reasoning models include:</p> <ul> <li>DeepSeek R1, </li> <li>o3 and o1</li> <li>Ph-4-reasoning.</li> </ul>"},{"location":"lab-1-introduction-to-reasoning-models/#getting-started-with-deepseek-r1","title":"Getting started with DeepSeek R1","text":"<p>To access DeepSeek R1 we will be using langchain-azure-ai, the official Python package that contains most of Langchain's Azure integrations. This package leverages the Azure AI Inference SDK to give us access to all the models available in Azure AI Foundry using Langchain. </p>"},{"location":"lab-1-introduction-to-reasoning-models/#lab-exercise","title":"Lab Exercise","text":"<ol> <li> <p>Open the file lab1a_reasoning.py and examine the code in this file. This is all the code we need to access and run DeepSeek R1. </p> </li> <li> <p>In the VS Code terminal run the following command and enter a research topic. For example 'What's the best coffee in Seattle?'\u2615:</p> <pre><code>python lab1a_reasoning.py\n</code></pre> <p>What do you notice about the answer the model returns vs what a chat model would typically output? </p> <p>Type in 'exit' to leave the program and continue with the next section. </p> </li> </ol>"},{"location":"lab-1-introduction-to-reasoning-models/#seperating-thinking-from-the-final-answer","title":"Seperating Thinking from the Final Answer","text":"<p>The output from the model in the previous step contains the models thinking. This output might be helpful for developers but might not always be useful when building an application where users only expect the final output. </p> <p>Many reasoning models put their thought process in thinking tags that look like this  <code>&lt;think&gt; &lt;/think&gt;</code>. </p> <p>In order to seperate the models thinking from it's final answer we can create helper function in Python to seperate the thinking from the final output. The function would look like this:</p> <pre><code>def seperate_thinking_from_final_output(text: str):\n    \"\"\"\n    Extract the content between &lt;think&gt; and &lt;/think&gt; tags and remove them from the text.\n    \"\"\"\n    thoughts = \"\"\n    while \"&lt;think&gt;\" in text and \"&lt;/think&gt;\" in text:\n        start = text.find(\"&lt;think&gt;\")\n        end = text.find(\"&lt;/think&gt;\")\n        # Extract the content between tags (excluding the tags themselves)\n        thoughts += text[start + len(\"&lt;think&gt;\"):end].strip() + \"\\n\\n\"\n        # Remove the tags and their content from the original text\n        text = text[:start] + text[end + len(\"&lt;/think&gt;\"):]\n    return thoughts.strip(), text.strip()\n</code></pre>"},{"location":"lab-1-introduction-to-reasoning-models/#lab-exercise_1","title":"Lab Exercise","text":"<ol> <li> <p>Open the stream_llm_response.py file and examine the code in this file. It contains a slightly updated version of this helper function in the <code>stream_thinking_and_answer</code> function. This helper function will be imported in all our python files going forward. It also contains some code that uses the Python package rich for prettier display in the terminal. </p> </li> <li> <p>To test that this code works we can run the lab1b_reasoning.py script which imports the helper function. Run the following command in the terminal to do so:</p> <pre><code>python lab1b_reasoning.py\n</code></pre> </li> </ol>"},{"location":"lab-1-introduction-to-reasoning-models/#updating-the-system-prompt","title":"Updating the System Prompt","text":"<p>The final output is returned as bullet points. This format can be specified in the models system prompt that tells the LLM how to behave. The current system prompt in lab1b_reasoning.py is: </p> <pre><code>SYSTEM_PROMPT = \"\"\"\nYou are a research assistant that thinks carefully about questions before answering.\n\nWhen you receive a research question, first think about the problem step-by-step.\n\nAfter thinking, provide your final answer in bullet points.\nMake sure to include all the important details in your answer.\n</code></pre> <p>Note</p> <p>You do not need to add a system prompt but these can be useful for improving the format and contextual relevance of the models final response.</p>"},{"location":"lab-1-introduction-to-reasoning-models/#lab-exercise_2","title":"Lab Exercise","text":"<ol> <li> <p>Update the system prompt in lab1b_reasoning.py to change the final output format. For example ask the model to return the answer as a table or in a Q&amp;A format. </p> </li> <li> <p>Rerun the following command in the terminal to test your updates:</p> <pre><code>python lab1b_reasoning.py\n</code></pre> </li> </ol>"},{"location":"lab-1-introduction-to-reasoning-models/#next-steps","title":"Next Steps","text":"<p>Congratulations, you should now feel comfortable using reasoning models!  You are now ready to move on to Lab 2: Web Research Integration.</p>"},{"location":"lab-2-web-research/","title":"Lab 2: Web Research Integration","text":""},{"location":"lab-2-web-research/#why-should-we-connect-llms-to-the-web","title":"Why Should We Connect LLMs To The Web?","text":"<p>So far we have been using DeepSeek R1 to return an answer to our research questions. While LLMs have extensive knowledge, they have two major limitations:</p> <ol> <li>Training Data Cutoff: Models only have knowledge up to their training cutoff date</li> <li>Knowledge Limitations: No model knows everything, especially about niche or emerging topics</li> </ol> <p>By connecting LLMs to web search capabilities, you can:</p> <ul> <li>Obtain current information not available during model training</li> <li>Get authoritative information from reliable sources</li> <li>Research specialized topics where the model's knowledge may be limited</li> <li>Find specific data points, statistics, and references</li> </ul>"},{"location":"lab-2-web-research/#the-web-research-process","title":"The Web Research Process","text":"<p>To get the best web research results possible we will create a python script that involves three key components:</p> <ol> <li>Query Generation: Creating effective search queries based on the research topic</li> <li>Web Search: Retrieving relevant information from external sources</li> <li>Information Synthesis: Integrating the retrieved information into a cohesive summary</li> </ol> <p></p>"},{"location":"lab-2-web-research/#query-generation","title":"Query Generation","text":"<p>We will begin by using DeepSeek R1 to generate an optimal search query based on the users research topic.  As we saw earlier the system prompt lets the model know how to respond. For the query generator we want the model to return  the optimizes search query and a brief explanation of why this query is relevant. </p> <p>Note</p> <p>This system prompt and others can be found in the prompts.py file. </p> <p>The system prompt for the query writer is as follows:</p> <pre><code>query_writer_instructions=\"\"\"Your goal is to generate a targeted web search query.\n\n&lt;CONTEXT&gt;\nCurrent date: {current_date}\nPlease ensure your queries account for the most current information available as of this date.\n&lt;/CONTEXT&gt;\n\n&lt;TOPIC&gt;\n{research_topic}\n&lt;/TOPIC&gt;\n\n&lt;FORMAT&gt;\nFormat your response as a JSON object with ALL three of these exact keys:\n   - \"query\": The actual search query string\n   - \"rationale\": Brief explanation of why this query is relevant\n&lt;/FORMAT&gt;\n\n&lt;EXAMPLE&gt;\nExample output:\n{{\n    \"query\": \"machine learning transformer architecture explained\",\n    \"rationale\": \"Understanding the fundamental structure of transformer models\"\n}}\n&lt;/EXAMPLE&gt;\n\nProvide your response in JSON format. Do not include any tags or backticks. Only return\nJson like in the example:\"\"\"\n</code></pre>"},{"location":"lab-2-web-research/#web-search","title":"Web Search","text":"<p>The generated query is then used to perform a web search using the Tavily API.  This is the code required:</p> <pre><code>def perform_web_search(query):\n    \"\"\"Perform a web search using the Tavily API.\"\"\"\n\n    search_results = tavily_client.search(query)\n\n    for i, result in enumerate(search_results[\"results\"]):\n        print(f\"**Result {i + 1}**\")\n        print(f\"**Title**: {result['title']}\")\n        print(f\"**Snippet**: {result['content']}\")\n        print(f\"**URL**: {result['url']}\\n\")\n\n    return search_results\n</code></pre> <p>Tavily retrieves information from the web about the topic and displays snippets of the results.</p> <p>Tip</p> <p>Tavily offers several options to alter the quality and quantity of the search_results.  To specify the number of search results use the <code>max_results</code> parameter.  To determine the depth of the search set the <code>search_depth</code> parameter to <code>basic</code> or <code>advanced</code>.  Advanced returns higher quality results but takes longer. </p>"},{"location":"lab-2-web-research/#lab-excercise","title":"Lab Excercise","text":"<ol> <li> <p>Examine the code in lab2a_web_research.py.</p> </li> <li> <p>Run the following command in the terminal to try out web search:</p> <pre><code>python lab2a_web_research.py\n</code></pre> </li> <li> <p>Update the code to test out returning more than one result and test out the advanced search. </p> </li> </ol>"},{"location":"lab-2-web-research/#next-steps","title":"Next Steps","text":"<p>You have now learnt how to use a reasoining model with a search tool.  Move on to Lab 3: Research Reflection.</p>"},{"location":"lab-3-reflection/","title":"Lab 3: Research Reflection","text":"<p>In this lab, you'll learn how to create a more thorough research system by implementing knowledge gap identification and iterative research cycles. This advanced approach allows your research assistant to identify what it doesn't know and conduct follow-up research to create more comprehensive results.</p>"},{"location":"lab-3-reflection/#understanding-research-reflection","title":"Understanding Research Reflection","text":"<p>Even with web search capabilities, a single research cycle often leaves important questions unanswered. Real research is iterative - findings from initial searches reveal what else we need to know.</p> <p>Research reflection involves:</p> <ol> <li>Knowledge Gap Identification: Analyzing current information to find what's missing</li> <li>Follow-up Query Generation: Creating targeted follow-up questions to fill these gaps</li> <li>Multiple Research Cycles: Conducting iterative research to build comprehensive knowledge</li> <li>Progressive Synthesis: Combining findings from all iterations into a coherent whole</li> </ol>"},{"location":"lab-3-reflection/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll:</p> <ol> <li>Build upon the web search capabilities from Lab 2</li> <li>Implement knowledge gap analysis with AI reasoning</li> <li>Create an iterative research system that conducts multiple search cycles</li> <li>Generate comprehensive research reports combining multiple iterations</li> <li>See real-time streaming of the AI's thinking throughout the entire process</li> </ol>"},{"location":"lab-3-reflection/#the-iterative-research-process","title":"The Iterative Research Process","text":"<p>The complete iterative research process includes:</p> <ol> <li>Initial query generation and web search (from Lab 2)</li> <li>Knowledge gap identification after initial research</li> <li>Follow-up search cycles based on identified gaps</li> <li>Synthesis of all findings into a comprehensive report</li> </ol>"},{"location":"lab-3-reflection/#the-code","title":"The Code","text":"<p>The Python application in <code>lab3_reflection.py</code> extends our Lab 2 implementation with knowledge gap identification and iterative research:</p> <pre><code>import os\nimport json\nimport time\nimport dotenv\nfrom typing import Dict, List, Any, Tuple\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom tavily import TavilyClient\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.markdown import Markdown\nfrom rich.prompt import Prompt\nfrom rich.progress import Progress, SpinnerColumn, TextColumn\nfrom rich.live import Live\n\n# ... abbreviated for clarity ...\n</code></pre>"},{"location":"lab-3-reflection/#key-components","title":"Key Components","text":""},{"location":"lab-3-reflection/#knowledge-gap-identification","title":"Knowledge Gap Identification","text":"<p>After the initial research cycle, the system analyzes what information is missing:</p> <pre><code>def identify_knowledge_gaps(research_topic, current_summary):\n    \"\"\"Identify knowledge gaps and generate a follow-up query.\"\"\"\n    console.print(\"\\n[bold blue]Identifying knowledge gaps...[/]\")\n\n    messages = [\n        SystemMessage(content=REFLECTION_PROMPT),\n        HumanMessage(content=f\"Research Topic: {research_topic}\\n\\nCurrent Summary:\\n{current_summary}\")\n    ]\n\n    # Stream the model's thinking process for knowledge gap identification\n    console.print(\"\\n[bold]Knowledge Gap Analysis Process:[/]\\n\")\n    response_stream = model.stream(messages)\n    thoughts, json_str = stream_thinking_and_answer(response_stream, \"\ud83d\udd0d Reflection Thinking\")\n\n    # Try to parse the JSON response\n    try:\n        reflection = json.loads(json_str)\n        knowledge_gap = reflection.get(\"knowledge_gap\", \"No specific knowledge gap identified.\")\n        follow_up_query = reflection.get(\"follow_up_query\", f\"More information about {research_topic}\")\n    except json.JSONDecodeError:\n        # Fallback if JSON parsing fails\n        knowledge_gap = \"Unable to parse the identified knowledge gap.\"\n        follow_up_query = f\"More information about {research_topic}\"\n\n    display_panel(\n        f\"**Knowledge Gap**: {knowledge_gap}\\n\\n**Follow-up Query**: {follow_up_query}\",\n        \"\ud83d\udd0d Knowledge Gap Analysis\",\n        \"yellow\"\n    )\n\n    return knowledge_gap, follow_up_query\n</code></pre> <p>This function: - Analyzes the current research summary to identify knowledge gaps - Generates a specific follow-up query to address the most important gap - Returns both the knowledge gap description and the follow-up query - Streams the model's thinking process in real-time</p>"},{"location":"lab-3-reflection/#multi-iteration-research","title":"Multi-Iteration Research","text":"<p>The <code>conduct_research</code> function now performs multiple research iterations:</p> <pre><code>def conduct_research(research_topic, max_iterations=2):\n    \"\"\"Conduct multi-step research on a topic.\"\"\"\n    console.print(f\"[bold]Starting research on: [green]{research_topic}[/green][/]\\n\")\n\n    all_summaries = []\n\n    for iteration in range(1, max_iterations + 1):\n        console.print(f\"\\n[bold]===== Research Iteration {iteration} =====[/]\\n\")\n\n        # Generate search query (use the research topic for first iteration)\n        if iteration == 1:\n            query = generate_search_query(research_topic)\n        else:\n            # For subsequent iterations, use the follow-up query from reflection\n            _, query = identify_knowledge_gaps(research_topic, all_summaries[-1])\n\n        # Perform web search\n        search_results = perform_web_search(query)\n\n        # Summarize search results\n        summary = summarize_search_results(research_topic, search_results)\n        all_summaries.append(summary)\n\n    # Compile final research report from all summaries\n    console.print(\"\\n[bold]===== Final Research Report =====[/]\\n\")\n\n    final_report = f\"# Research Report: {research_topic}\\n\\n\"\n    for i, summary in enumerate(all_summaries, 1):\n        final_report += f\"## Research Cycle {i}\\n\\n{summary}\\n\\n\"\n\n    display_panel(final_report, \"\ud83d\udcca Complete Research Report\", \"purple\")\n\n    return final_report\n</code></pre> <p>This function: - Performs multiple research iterations (default is 2) - Uses the initial topic for the first search - Uses knowledge gap analysis to guide subsequent searches - Compiles all findings into a comprehensive final report</p>"},{"location":"lab-3-reflection/#research-report-compilation","title":"Research Report Compilation","text":"<p>The final report combines findings from all research iterations:</p> <pre><code># Compile final research report from all summaries\nconsole.print(\"\\n[bold]===== Final Research Report =====[/]\\n\")\n\nfinal_report = f\"# Research Report: {research_topic}\\n\\n\"\nfor i, summary in enumerate(all_summaries, 1):\n    final_report += f\"## Research Cycle {i}\\n\\n{summary}\\n\\n\"\n\ndisplay_panel(final_report, \"\ud83d\udcca Complete Research Report\", \"purple\")\n</code></pre> <p>This creates a well-structured report with sections for each research cycle.</p>"},{"location":"lab-3-reflection/#running-the-application","title":"Running the Application","text":"<p>To run the application:</p> <ol> <li>Ensure you have the required packages installed:</li> </ol> <pre><code>pip install python-dotenv langchain-azure-ai tavily-python rich\n</code></pre> <ol> <li>Make sure your <code>.env</code> file includes both Azure OpenAI and Tavily API keys:</li> </ol> <pre><code>AZURE_AI_ENDPOINT=your_azure_openai_endpoint\nAZURE_API_KEY=your_azure_openai_key\nTAVILY_API_KEY=your_tavily_api_key\n</code></pre> <ol> <li>Run the script:</li> </ol> <pre><code>python lab3_reflection.py\n</code></pre> <ol> <li>Enter a research topic and observe the multi-cycle research process.</li> </ol>"},{"location":"lab-3-reflection/#example-usage","title":"Example Usage","text":"<p>When you run the application and enter a research topic like \"quantum computing applications in medicine\", you'll see:</p> <ol> <li>First Research Cycle:</li> <li>Generation of an initial search query</li> <li>Web search results</li> <li> <p>Summary of initial findings</p> </li> <li> <p>Knowledge Gap Analysis:</p> </li> <li>Identification of missing information</li> <li> <p>Generation of a follow-up query</p> </li> <li> <p>Second Research Cycle:</p> </li> <li>Web search using the follow-up query</li> <li> <p>Summary of additional findings</p> </li> <li> <p>Final Research Report:</p> </li> <li>Comprehensive report combining all research cycles</li> </ol>"},{"location":"lab-3-reflection/#benefits-of-iterative-research","title":"Benefits of Iterative Research","text":"<p>This approach offers several advantages:</p> <ol> <li>Thoroughness: Multiple research cycles produce more comprehensive results</li> <li>Targeted Follow-up: Each cycle focuses on what's still unknown</li> <li>Progressive Refinement: Later cycles build on knowledge from earlier ones</li> <li>Better Coverage: Different search queries capture different aspects of the topic</li> <li>Self-Awareness: The system acknowledges what it doesn't know</li> </ol>"},{"location":"lab-3-reflection/#lab-challenges","title":"Lab Challenges","text":"<p>Try these challenges to extend your learning:</p> <ol> <li>Increase Iterations: Modify the code to perform 3 or more research cycles</li> <li>Topic Segmentation: Modify the system to explore different subtopics in parallel</li> <li>User Guidance: Add user input between iterations to guide the research direction</li> <li>Sentiment Analysis: Add analysis of different perspectives on controversial topics</li> <li>Citation Improvement: Create a more formal citation system for sources</li> </ol>"},{"location":"lab-3-reflection/#key-takeaways","title":"Key Takeaways","text":"<p>From this lab, you should understand:</p> <ul> <li>How to implement knowledge gap identification</li> <li>Techniques for creating targeted follow-up queries</li> <li>Methods for conducting multi-cycle research</li> <li>The value of iterative approaches in comprehensive research</li> </ul>"},{"location":"lab-3-reflection/#next-steps","title":"Next Steps","text":"<p>Ready to incorporate this research system into a full production application? Move on to Lab 4: Launching Your Researcher.</p>"},{"location":"lab-4-launch-researcher/","title":"Lab 4: Deploying Your Research Assistant","text":"<p>In this lab, you'll transform your terminal-based research assistant into a professional web application using FastAPI and WebSockets. This web interface provides a more user-friendly experience with real-time research updates and a polished UI.</p>"},{"location":"lab-4-launch-researcher/#understanding-web-application-deployment","title":"Understanding Web Application Deployment","text":"<p>Moving from a command-line tool to a web application offers several advantages:</p> <ol> <li>User Accessibility: Anyone can use the application through a web browser</li> <li>Real-time Updates: WebSockets enable live streaming of research progress</li> <li>Professional Presentation: A well-designed UI enhances the research experience</li> <li>Scalability: The application can be deployed to cloud services for broader access</li> </ol>"},{"location":"lab-4-launch-researcher/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll:</p> <ol> <li>Learn about the FastAPI web framework</li> <li>Set up a WebSocket connection for real-time streaming</li> <li>Create a research workflow using a state graph</li> <li>Deploy a responsive web interface</li> <li>See all previous lab techniques integrated into a production-ready application</li> </ol>"},{"location":"lab-4-launch-researcher/#architecture-overview","title":"Architecture Overview","text":"<p>The web application uses a modern architecture:</p> <ol> <li>Backend: FastAPI Python server with WebSocket support</li> <li>Frontend: HTML/CSS/JavaScript with Tailwind CSS for styling</li> <li>State Management: LangGraph for structured AI workflow orchestration</li> <li>Data Flow: Real-time bidirectional communication via WebSockets</li> </ol> <p></p>"},{"location":"lab-4-launch-researcher/#key-components","title":"Key Components","text":""},{"location":"lab-4-launch-researcher/#fastapi-application","title":"FastAPI Application","text":"<p>The main application in <code>app/main.py</code> serves as the backend server:</p> <pre><code>from fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom langgraph.graph import StateGraph, START, END\n# ... other imports ...\n\napp = FastAPI(title=\"Azure Deep Research\")\n\n# Mount static files\napp.mount(\"/static\", StaticFiles(directory=\"app/static\"), name=\"static\")\n\n# Set up templates\ntemplates = Jinja2Templates(directory=\"app/templates\")\n</code></pre>"},{"location":"lab-4-launch-researcher/#research-graph-with-langgraph","title":"Research Graph with LangGraph","text":"<p>The application uses LangGraph to create a structured research workflow:</p> <pre><code>def setup_graph():\n    # Add nodes and edges\n    builder = StateGraph(SummaryState, input=SummaryStateInput, output=SummaryStateOutput)\n    builder.add_node(\"generate_query\", generate_query)\n    builder.add_node(\"web_research\", web_research)\n    builder.add_node(\"summarize_sources\", summarize_sources)\n    builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n    builder.add_node(\"finalize_summary\", finalize_summary)\n\n    # Add edges\n    builder.add_edge(START, \"generate_query\")\n    builder.add_edge(\"generate_query\", \"web_research\")\n    builder.add_edge(\"web_research\", \"summarize_sources\")\n    builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n    builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n    builder.add_edge(\"finalize_summary\", END)\n\n    return builder.compile()\n</code></pre> <p>This creates a research graph with: 1. Query generation 2. Web research retrieval 3. Information summarization 4. Knowledge gap reflection 5. Multiple research cycles 6. Final report generation</p>"},{"location":"lab-4-launch-researcher/#websocket-communication","title":"WebSocket Communication","text":"<p>Real-time updates are provided through WebSockets:</p> <pre><code>@app.websocket(\"/ws/{client_id}\")\nasync def websocket_endpoint(websocket: WebSocket, client_id: str):\n    await websocket.accept()\n    manager.connect(websocket, client_id)\n    try:\n        # Set up the graph\n        graph = setup_graph()\n\n        while True:\n            data = await websocket.receive_text()\n            data_json = json.loads(data)\n\n            if data_json.get(\"type\") == \"research\":\n                # ... start research process ...\n\n                async def stream_graph_updates():\n                    # ... stream updates to client ...\n\n                # Run graph execution in the background\n                asyncio.create_task(stream_graph_updates())\n\n    except WebSocketDisconnect:\n        manager.disconnect(client_id)\n</code></pre>"},{"location":"lab-4-launch-researcher/#interactive-web-interface","title":"Interactive Web Interface","text":"<p>The web interface in <code>app/templates/index.html</code> provides a clean, responsive UI:</p> <ul> <li>Research topic input</li> <li>Real-time progress indicators</li> <li>Live streaming of research steps</li> <li>Formatted research report display</li> <li>\"AI Thinking\" modal to show reasoning process</li> </ul>"},{"location":"lab-4-launch-researcher/#running-the-application","title":"Running the Application","text":"<p>To launch the web application:</p> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the FastAPI server:</li> </ol> <pre><code>uvicorn app.main:app --reload\n</code></pre> <ol> <li>Open your browser and navigate to:</li> </ol> <pre><code>http://localhost:8000\n</code></pre>"},{"location":"lab-4-launch-researcher/#using-the-web-interface","title":"Using the Web Interface","text":"<p>The web application features a clean, intuitive interface:</p> <ol> <li>Research Input: Enter a research topic in the input field and click \"Research\"</li> <li>Progress Tracking: Watch as the system progresses through each research step</li> <li>Live Updates: See real-time updates as the research is conducted</li> <li>Thinking Process: Click the thought bubble icon to view the AI's reasoning process</li> <li>Final Report: View the comprehensive research report with citations</li> </ol>"},{"location":"lab-4-launch-researcher/#behind-the-scenes","title":"Behind the Scenes","text":"<p>When a user submits a research topic, the application:</p> <ol> <li>Initializes a WebSocket connection for real-time communication</li> <li>Creates a research graph using LangGraph</li> <li>Executes the research workflow in a structured way:</li> <li>Generates an effective search query with visible thinking</li> <li>Performs web searches using the Tavily API</li> <li>Synthesizes information from search results</li> <li>Identifies knowledge gaps and creates follow-up queries</li> <li>Conducts multiple research cycles (up to 3 by default)</li> <li>Compiles a final research report with images and sources</li> <li>Streams progress updates to the client in real-time</li> <li>Displays the final research report with proper formatting</li> </ol>"},{"location":"lab-4-launch-researcher/#deployment-options","title":"Deployment Options","text":"<p>After testing locally, you can deploy the application to various platforms:</p>"},{"location":"lab-4-launch-researcher/#azure-app-service","title":"Azure App Service","text":"<ol> <li>Create an Azure App Service with Python support</li> <li>Set up your environment variables in the App Service configuration</li> <li>Deploy your code using Azure DevOps, GitHub Actions, or direct deployment</li> </ol>"},{"location":"lab-4-launch-researcher/#docker-deployment","title":"Docker Deployment","text":"<ol> <li>Create a Dockerfile in your project root:</li> </ol> <pre><code>FROM python:3.10\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <ol> <li>Build and run the Docker container:</li> </ol> <pre><code>docker build -t azure-deep-research .\ndocker run -p 8000:8000 -d azure-deep-research\n</code></pre>"},{"location":"lab-4-launch-researcher/#enhancing-your-application","title":"Enhancing Your Application","text":"<p>Consider these enhancements for your research assistant:</p> <ol> <li>User Authentication: Add user accounts and save research history</li> <li>Database Integration: Store research reports for later reference</li> <li>Research Export: Add options to export as PDF or other formats</li> <li>Custom Research Settings: Allow users to adjust research depth and sources</li> <li>Document Upload: Enable research based on uploaded documents</li> </ol>"},{"location":"lab-4-launch-researcher/#lab-challenges","title":"Lab Challenges","text":"<p>Try these challenges to extend your learning:</p> <ol> <li>Add Visualization: Integrate data visualization for numerical research topics</li> <li>Implement Caching: Add result caching to improve performance</li> <li>Multi-Language Support: Add translation capabilities for multiple languages</li> <li>Citation Management: Implement formal academic citation formats</li> <li>Image Analysis: Add capabilities to analyze images found during research</li> </ol>"},{"location":"lab-4-launch-researcher/#key-takeaways","title":"Key Takeaways","text":"<p>From this lab, you should understand:</p> <ul> <li>How to deploy an AI research assistant as a web application</li> <li>Using WebSockets for real-time communication</li> <li>Implementing a structured workflow with LangGraph</li> <li>Creating an interactive user interface for research</li> <li>Integrating all previous lab techniques into a production application</li> </ul>"},{"location":"lab-4-launch-researcher/#congratulations","title":"Congratulations!","text":"<p>You've successfully built and deployed a sophisticated AI research assistant using Azure OpenAI, LangGraph, and FastAPI. This application demonstrates modern AI techniques including:</p> <ul> <li>Transparent thinking and reasoning</li> <li>Web search integration</li> <li>Knowledge gap identification</li> <li>Iterative research cycles</li> <li>Real-time progress streaming</li> </ul> <p>Your application now provides a powerful research tool that can help users explore any topic with the assistance of advanced AI capabilities.</p>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#workshop-repository-and-documentation","title":"Workshop Repository and Documentation","text":"<p>The workshop repository on GitHub contains everything needed for the workshop. The repo includes the source code (src folder) for the sample application, the documentation (docs) built with MkDocs, and the Azure deployment resources (infra) for the workshop.</p> <ul> <li>Repository: microsoft/BUILD25-LAB331/tree/main</li> </ul>"},{"location":"resources/#microsoft-learn-resources","title":"Microsoft Learn resources","text":"<ul> <li>Azure AI Foundry </li> <li>LangGraph Documentation</li> <li>Tavily Python SDK</li> </ul>"},{"location":"resources/#deep-researcher-sample","title":"Deep Researcher sample","text":"<p>Find the official Deep Research Sample here: </p>"},{"location":"resources/#thank-you","title":"Thank you","text":"<p>Thank you for participating in this workshop! If you have any suggestions for improvements or encountered any problems while running this workshop, please let us know via GitHub Issues.</p>"},{"location":"summary/","title":"Workshop Summary","text":"<p>Congratulations on completing the Azure Deep Research Workshop! Let's recap what you've learned and built throughout this journey.</p>"},{"location":"summary/#what-youve-built","title":"What You've Built","text":"<p>Over the course of this workshop, you've constructed a sophisticated AI research assistant that combines several cutting-edge technologies:</p> <ul> <li>Azure AI Service: Leveraging DeepSeek-R1 for powerful reasoning</li> <li>LangChain &amp; LangGraph: Structuring complex AI workflows with state management</li> <li>Tavily API: Integrating real-time web search capabilities</li> <li>FastAPI &amp; WebSockets: Creating a responsive web application with real-time updates</li> </ul> <p>Your research assistant now features:</p> <ol> <li>Transparent AI Reasoning: Visible thinking processes that show how the AI arrives at conclusions</li> <li>Multi-stage Research: Query generation, web search, and information synthesis</li> <li>Knowledge Gap Detection: Identification of missing information and follow-up research</li> <li>Iterative Refinement: Multiple research cycles to build comprehensive knowledge</li> <li>Real-time Progress Updates: WebSocket streaming of each research step</li> <li>Professional Web Interface: Clean, responsive UI for an excellent user experience</li> </ol>"},{"location":"summary/#key-concepts-learned","title":"Key Concepts Learned","text":""},{"location":"summary/#lab-1-reasoning-model-thoughts","title":"Lab 1: Reasoning &amp; Model Thoughts","text":"<ul> <li>Separating AI thinking from final outputs</li> <li>Implementing real-time thought streaming</li> <li>Creating structured response formats</li> <li>Using system prompts to guide model behavior</li> </ul>"},{"location":"summary/#lab-2-web-research-integration","title":"Lab 2: Web Research Integration","text":"<ul> <li>Generating effective search queries from research topics</li> <li>Integrating web search APIs for real-time information</li> <li>Processing and synthesizing search results</li> <li>Creating informative summaries from diverse sources</li> </ul>"},{"location":"summary/#lab-3-research-reflection","title":"Lab 3: Research Reflection","text":"<ul> <li>Implementing knowledge gap identification</li> <li>Creating research reflection capabilities</li> <li>Building iterative research cycles</li> <li>Managing research state across multiple steps</li> </ul>"},{"location":"summary/#lab-4-launching-your-researcher","title":"Lab 4: Launching Your Researcher","text":"<ul> <li>Building a FastAPI application with WebSocket support</li> <li>Implementing real-time client-server communication</li> <li>Creating a responsive web interface with Tailwind CSS</li> <li>Structuring a complete research workflow with LangGraph</li> </ul>"},{"location":"summary/#architectural-patterns","title":"Architectural Patterns","text":"<p>Throughout this workshop, you've implemented several important architectural patterns:</p> <ol> <li>Streaming Architecture: Real-time data flow between components</li> <li>State Management: Tracking and updating research state</li> <li>Component Separation: Modular design with separated concerns</li> <li>API Integration: Connecting to external services</li> <li>Asynchronous Processing: Non-blocking operations for better performance</li> <li>Reactive UI: Interface that responds to real-time updates</li> </ol>"},{"location":"summary/#final-thoughts","title":"Final Thoughts","text":"<p>The AI research assistant you've built represents a powerful tool that combines the strengths of large language models with structured workflows and web capabilities. This architecture can be adapted to many different use cases beyond research, including:</p> <ul> <li>Customer support systems</li> <li>Educational assistants</li> <li>Content creation tools</li> <li>Data analysis assistants</li> <li>Decision support systems</li> </ul> <p>By understanding the core components and patterns, you can extend this framework to solve a wide range of problems that require intelligent information processing, reasoning, and presentation.</p> <p>Thank you for participating in this workshop. We hope you've gained valuable skills that will help you build sophisticated AI applications in the future!</p>"},{"location":"includes/introduction-event/","title":"Introduction event","text":""},{"location":"includes/introduction-event/#microsoft-build-attendees","title":"Microsoft Build Attendees","text":"<p>The instructions on this page assume you are attending Microsoft Build 2025 and have access to a pre-configured lab environment. This environment provides an Azure subscription with all the tools and resources needed to complete the workshop.</p>"},{"location":"includes/introduction-event/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you how to use Reasoning Models, like DeepSeek R1 and utilize tools and Reflection style architecture with LangChain to do deep research. It consists of multiple labs, each highlighting a specific feature of the process of building a deep researcher. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"includes/introduction-event/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure so the agent app can access the Azure AI Agents Service and models. Follow these steps:</p> <ol> <li> <p>Open a terminal window. The terminal app is pinned to the Windows 11 taskbar.</p> <p></p> </li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account.</p> <ol> <li> <p>A browser window will open automatically, select Work or school account and click Next.</p> </li> <li> <p>Use the Username and Password found in the top section of the Resources tab in the lab environment.</p> </li> <li> <p>Select OK, then Done.</p> </li> </ol> </li> <li> <p>Then select the Default subscription from the command line.</p> </li> <li> <p>Once you've logged in, run the following command to assign the user role to the resource group:</p> <pre><code>$subId = $(az account show --query id --output tsv) `\n;$objectId = $(az ad signed-in-user show --query id -o tsv) `\n; az role assignment create --role \"f6c7c914-8db3-469d-8ca1-694a8f32e121\" --assignee-object-id $objectId --scope /subscriptions/$subId/resourceGroups/\"rg-agent-workshop\" --assignee-principal-type 'User'\n</code></pre> </li> <li> <p>Leave the terminal window open for the next steps.</p> </li> </ol>"},{"location":"includes/introduction-event/#open-the-workshop","title":"Open the Workshop","text":"<p>Follow these steps to open the workshop in Visual Studio Code:</p> Python <ol> <li> <p>From the terminal window, execute the following commands to clone the workshop repository, navigate to the relevant folder, set up a virtual environment, activate it, and install the required packages:</p> <pre><code>git clone https://github.com/microsoft/BUILD25-LAB331.git `\n; cd src `\n; python -m venv src/python/workshop/.venv `\n; src\\.venv\\Scripts\\activate `\n; pip install -r src/requirements.txt `\n</code></pre> </li> <li> <p>Open in VS Code. From the terminal window, run the following command:</p> <pre><code>code .vscode\\python-workspace.code-workspace\n</code></pre> <p>When the project opens in VS Code, two notifications appear in the bottom right corner. Click \u2716 to close both notifications.</p> </li> </ol>"},{"location":"includes/introduction-event/#project-connection-string","title":"Project Connection String","text":"<p>Next, we log in to Azure AI Foundry to retrieve the project connection string, which the agent app uses to connect to the Azure AI Agents Service.</p> <ol> <li>Navigate to the Azure AI Foundry website.</li> <li>Select Sign in and use the Username and Password found in the top section of the Resources tab in the lab environment. Click on the Username and Password fields to automatically fill in the login details.     </li> <li>Read the introduction to the Azure AI Foundry and click Got it.</li> <li> <p>Ensure you are on the AI Foundry home page. Click the AI Foundry tab in the top left corner.</p> <p></p> </li> <li> <p>Select the project name that starts with aip-.</p> <p></p> </li> <li> <p>Review the introduction guide and click Close.</p> </li> <li> <p>Locate the Project details section, click the Copy icon to copy the Project connection string.</p> <p></p> </li> </ol>"},{"location":"includes/introduction-event/#configure-the-workshop","title":"Configure the Workshop","text":"<pre><code>1. Switch back to the workshop you opened in VS Code.\n2. **Rename** the `.env.sample` file to `.env`.\n\n    - Select the **.env.sample** file in the VS Code **Explorer** panel.\n    - Right-click the file and select **Rename**, or press &lt;kbd&gt;F2&lt;/kbd&gt;.\n    - Change the file name to `.env` and press &lt;kbd&gt;Enter&lt;/kbd&gt;.\n\n3. Paste the **Project connection string** you copied from Azure AI Foundry into the `.env` file.\n\n    ```python\n    PROJECT_CONNECTION_STRING=\"&lt;your_project_connection_string&gt;\"\n    ```\n\n    Your `.env` file should look similar to this but with your project connection string.\n\n    ```python\n    MODEL_DEPLOYMENT_NAME=\"DeepSeek-R1\"\n    PROJECT_CONNECTION_STRING=\"&lt;your_project_connection_string&gt;\"\n    ```\n\n4. Save the `.env` file.\n</code></pre>"},{"location":"includes/introduction-event/#5-set-up-tavily-api","title":"5. Set Up Tavily API","text":"<pre><code>1. Register for a [Tavily API key](https://tavily.com/) to enable web search capabilities. This API is essential for the web research integration in Labs 2 and 3.\n\n2. Update the `.env` file in the project root with your Tavily API keys. Your file should now look like this:\n\n```\nAZURE_AI_ENDPOINT=your_azure_openai_endpoint\nAZURE_API_KEY=your_azure_openai_key\nTAVILY_API_KEY=your_tavily_api_key\n```\n</code></pre>"},{"location":"includes/introduction-event/#workshop-materials","title":"Workshop Materials","text":"<p>This workshop is designed as a progressive series of labs, each building on the previous one:</p> <ol> <li> <p>Lab 1 (Reasoning &amp; Model Thoughts): Learn how to stream the AI's thinking process in real-time and format final answers as bullet points.</p> </li> <li> <p>Lab 2 (Web Research Integration): Build a basic research system that generates queries, searches the web, and synthesizes the results.</p> </li> <li> <p>Lab 3 (Research Reflection): Extend the research capabilities with knowledge gap identification and iterative research cycles.</p> </li> <li> <p>Lab 4 (Launching Your Researcher): Deploy the research assistant as a web application with FastAPI and WebSockets for real-time updates.</p> </li> </ol> <p>Each lab includes code samples, explanations, and challenges to extend your learning.</p>"},{"location":"includes/introduction-event/#navigation","title":"Navigation","text":"<p>Use the navigation menu on the left to move between labs. Each lab includes:</p> <ul> <li>An overview of what you'll learn</li> <li>Prerequisites specific to that lab</li> <li>Detailed instructions with code samples</li> <li>Explanations of key components</li> <li>Challenges to extend your learning</li> <li>Next steps to progress through the workshop</li> </ul>"},{"location":"includes/introduction-event/#pro-tips","title":"Pro Tips","text":"<p>Tips</p> <ol> <li>The Burger Menu in the right-hand panel of the lab environment offers additional features, including the Split Window View and the option to end the lab. The Split Window View allows you to maximize the lab environment to full screen, optimizing screen space. The lab's Instructions and Resources panel will open in a separate window.</li> <li>If the lab instructions are slow to scroll in the lab environment, try copying the instructions\u2019 URL and opening it in your computer\u2019s local browser for a smoother experience.</li> <li>If you have trouble viewing an image, simply click the image to enlarge it.</li> </ol>"},{"location":"includes/introduction-self-guided/","title":"Introduction self guided","text":""},{"location":"includes/introduction-self-guided/#self-guided-learners","title":"Self-Guided Learners","text":"<p>These instructions are for self-guided learners who do not have access to a pre-configured lab environment. Follow these steps to set up your environment and begin the workshop.</p>"},{"location":"includes/introduction-self-guided/#introduction","title":"Introduction","text":"<p>This workshop is designed to teach you how to use Reasoning Models, like DeepSeek R1 and utilize tools and Reflection style architecture with LangChain to do deep research. It consists of multiple labs, each highlighting a specific feature of the process of building a deep researcher. The labs are meant to be completed in order, as each one builds on the knowledge and work from the previous lab.</p>"},{"location":"includes/introduction-self-guided/#prerequisites","title":"Prerequisites","text":"<ol> <li>Access to an Azure subscription. If you don't have an Azure subscription, create a free account before you begin.</li> <li>You need a GitHub account. If you don\u2019t have one, create it at GitHub.</li> </ol>"},{"location":"includes/introduction-self-guided/#open-the-workshop","title":"Open the Workshop","text":"<p>The preferred way to run this workshop is using GitHub Codespaces. This option provides a pre-configured environment with all the tools and resources needed to complete the workshop. Alternatively, you can open the workshop locally using a Visual Studio Code Dev Container.</p> GitHub Codespaces <p>Select Open in GitHub Codespaces to open the project in GitHub Codespaces.</p> <p></p> <p>Building the Codespace will take several minutes. You can continue reading the instructions while it builds.</p>"},{"location":"includes/introduction-self-guided/#authenticate-with-azure","title":"Authenticate with Azure","text":"<p>You need to authenticate with Azure so the agent app can access the Azure AI Agents Service and models. Follow these steps:</p> <ol> <li>Ensure the Codespace has been created.</li> <li>In the Codespace, open a new terminal window by selecting Terminal &gt; New Terminal from the VS Code menu.</li> <li> <p>Run the following command to authenticate with Azure:</p> <pre><code>az login --use-device-code\n</code></pre> <p>Note</p> <p>You'll be prompted to open a browser link and log in to your Azure account. Be sure to copy the authentication code first.</p> <ol> <li>A browser window will open automatically, select your account type and click Next.</li> <li>Sign in with your Azure subscription Username and Password.</li> <li>Paste the authentication code.</li> <li>Select OK, then Done.</li> </ol> <p>Warning</p> <p>If you have multiple Azure tenants, then you will need to select the appropriate tenant when authenticating.</p> <pre><code>az login --use-device-code --tenant &lt;tenant_id&gt;\n</code></pre> </li> <li> <p>Next, select the appropriate subscription from the command line.</p> </li> <li>Leave the terminal window open for the next steps.</li> </ol>"},{"location":"includes/introduction-self-guided/#deploy-the-azure-resources","title":"Deploy the Azure Resources","text":"<p>The following resources will be created in the <code>rg-deep-research-workshop</code> resource group in your Azure subscription.</p> <ul> <li>An Azure AI Foundry hub named deep-research-wksp</li> <li>An Azure AI Foundry project named Deep Research Workshop</li> <li>A Serverless (pay-as-you-go) DeepSeek R1 model deployment. See pricing details here.</li> </ul> <p>We have provided a bash script to automate the deployment of the resources required for the workshop. Alternatively, you may deploy resources manually using Azure AI Foundry studio. Select the desired tab.</p>"}]}